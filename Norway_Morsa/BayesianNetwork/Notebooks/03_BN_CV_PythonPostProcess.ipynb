{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Notebook takes observed and predicted lake chemistry/ecol values output by notebook 02_BN_development_1Season (from the continuous network cross validation section). These are then discretized into WFD-related boundaries, and classification error is calculated. This classification error can then be compared to the classification error obtained by cross validation of discrete networks using the same classifications, or just to provide supporting error info to accompany predictions of probabilities of being in a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for processing below\n",
    "\n",
    "# Boundaries dictionary, copied from notebook B_seasonal_data_matrix_1Season\n",
    "bound_dict = {\n",
    "             'TP': [29.5], # No data below 20, so drop this class boundary. 29.5 is middle of 'Mod' class   \n",
    "             'chla': [20.0],  # WFD boundaries: [10.5, 20.0]. But only 6 d.p. under 10.5 so merge G and M classes.\n",
    "                              # For predicting cyano, would be better 17.4.   \n",
    "             'colour': [51.2], # [51.2] if comparing CV scores of continuous and discrete. [48.0] if calculating CV score of final network (665h percentile)\n",
    "             'cyano': [1.0], # M-P boundary is 2.0, but there were only 2 values in this class. Plenty above 2 tho.\n",
    "             }\n",
    "\n",
    "def discretize(thresholds, value):\n",
    "    \"\"\"\n",
    "    Function to compare a number to a list of thresholds and categorise accordingly. E.g. to apply row-wise down a df\n",
    "    Input:\n",
    "        thresholds: list of boundaries that define classes of interests\n",
    "        value: float to be compared to the thresholds\n",
    "    Returns: class the value lies in. Classes are defined in factor_li_dict within function according to number of thresholds\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.isnan(value):\n",
    "        return np.NaN\n",
    "    \n",
    "    factor_li_dict = {2: ['0','1'],\n",
    "                     3: ['0','1','2'],}\n",
    "    \n",
    "    n_classes = len(thresholds)+1\n",
    "    \n",
    "    for i, boundary in enumerate(thresholds):\n",
    "    \n",
    "        if value<boundary:\n",
    "            return factor_li_dict[n_classes][i]\n",
    "            break # Break out of loop\n",
    "        \n",
    "        # If we're up to the last class boundary, and the value is bigger than it, value is in the uppermost class\n",
    "        if i+1 == len(thresholds) and value >= boundary:\n",
    "            return factor_li_dict[n_classes][i+1]\n",
    "        \n",
    "def classification_error(df):\n",
    "    \"\"\"\n",
    "    Calculate classification error, the proportion of time the modle predicted the class correctly\n",
    "    \n",
    "    Input:\n",
    "        df: dataframe with two columns, 'obs' and 'pred'\n",
    "    Output:\n",
    "        Classification error (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # Were observed and predictions in the same class? 'right' col is a boolean, 1=Yes the same, 0=no different\n",
    "    right = np.where((df['obs'] == df['pred']), 1, 0)\n",
    "\n",
    "    classification_error = 1 - (right.sum() / len(right))\n",
    "    return classification_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xval_postprocess(var, fpath):\n",
    "    \"\"\"\n",
    "    Function to read in a csv of observed and predicted values from a continuous\n",
    "    Bayesian belief network produced in BNLearn R notebook, and calculate correlation\n",
    "    coefficients, and then classify according to WFD and work out classification error\n",
    "    for comparison to classification error calculated directly in BNLearn using discrete network.\n",
    "    \n",
    "    Inputs:\n",
    "        var: string, one of 'TP','chla','cyano','colour_summer'\n",
    "        fpath: string giving location of csv to be read in. csv should have columns:\n",
    "                'obs_1','pred_1','obs_2','pred_2',... where _1, _2, etc. is the cross\n",
    "                validation run number.\n",
    "    Returns:\n",
    "    Printed output, plus dictionary of results, with keys\n",
    "        'corr_coeffs': series of correlation coefficients, one value per cross validation run\n",
    "        'classification_errors': series of classification errors, one value per cross validation run\n",
    "        'cont_data_dict': dict of observed and predicted dfs, continuous data, one df per xval run\n",
    "        'classified_data_dict': dict of observed and predicted dfs, classified data, one df per xval run\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Read in data\n",
    "    df = pd.read_csv(fpath, index_col=0)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Split into separate dataframes for each cross validation run\n",
    "    cont_dict = {}  # Key: run number, returns df with obs and pred\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        if i % 2 == 0:  # If even, i.e. only do this for half the cols\n",
    "            run_no = int(col_name.split(\"_\", 1)[1])\n",
    "            if run_no == 1:\n",
    "                temp_df = df.iloc[:, [0, 1]]\n",
    "            else:\n",
    "                temp_df = df.iloc[:, [2 * run_no - 2, 2 * run_no - 1]]\n",
    "\n",
    "            temp_df.columns = [\"obs\", \"pred\"]\n",
    "        cont_dict[run_no] = temp_df\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Calculate correlation coefficients, convert to WFD classes and work out classification error\n",
    "\n",
    "    cc_dict = {}  # key: run_no, returns correlation coeff\n",
    "    mse_dict = {}  # key: run_no, returns mse (pred - obs)\n",
    "    classified_dict = {} # Key: run number. Returns df with cols 'obs' and 'pred', discrete data\n",
    "    class_error_dict = {}\n",
    "\n",
    "    for run_no in cont_dict.keys():\n",
    "\n",
    "        cont_df = cont_dict[run_no]  # df with continuous data\n",
    "\n",
    "        # Correlation coefficients\n",
    "        cc_dict[run_no] = cont_df[\"obs\"].corr(cont_df[\"pred\"], method=\"pearson\")\n",
    "\n",
    "        # Mean square error\n",
    "        mse = np.mean(((cont_df[\"pred\"] - cont_df[\"obs\"]) ** 2))\n",
    "        mse_dict[run_no] = mse\n",
    "\n",
    "        # Classify obs and pred into WFD (or related) class boundaries\n",
    "        disc_df = pd.DataFrame(index=cont_df.index, columns=cont_df.columns)  # New empty df to be populated\n",
    "        for col in cont_df.columns:\n",
    "            disc_df[col] = cont_df[col].apply(lambda x: discretize(bound_dict[var], x))\n",
    "        classified_dict[run_no] = disc_df\n",
    "\n",
    "        # Calculate classification error (proportion of time model predicted class correctly)\n",
    "        error = classification_error(disc_df)\n",
    "        class_error_dict[run_no] = error\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Aggregate results over runs\n",
    "\n",
    "    corr_coeffs = pd.Series(cc_dict)  # These match those calculated by bnlearn, good!\n",
    "    mses = pd.Series(mse_dict)\n",
    "    errors = pd.Series(class_error_dict)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Take a look at the output\n",
    "#     print(\"Correlation coefficients, %s:\" % var)\n",
    "#     print(corr_coeffs)\n",
    "    print(\"\\nMean correlation coefficient, %s: %s\" % (var, corr_coeffs.mean()))\n",
    "\n",
    "#     print(\"\\nmse, %s:\" % var)\n",
    "#     print(mses)\n",
    "    print(\"Mean mse, %s: %s\" % (var, mses.mean()))\n",
    "\n",
    "#     print(\"\\nClassification errors, %s:\" % var)\n",
    "#     print(errors)\n",
    "    print(\"Mean classification error for %s: %s\" % (var, errors.mean()))\n",
    "\n",
    "    # Return dictionaries\n",
    "    results_dict = {\n",
    "                    \"corr_coeffs\": corr_coeffs,\n",
    "                    \"classification_errors\": errors,\n",
    "                    \"cont_data_dict\": cont_dict,\n",
    "                    \"classified_data_dict\": classified_dict,\n",
    "                    }\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, TP: 0.7181037536758927\n",
      "Mean mse, TP: 11.185997265243895\n",
      "Mean classification error for TP: 0.3210526315789474\n"
     ]
    }
   ],
   "source": [
    "# With wind, all nodes\n",
    "fpath = \"../Data/CrossValidation/TP_continuous_fromAllNodes.csv\"\n",
    "results_dict = xval_postprocess(\"TP\", fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, TP: 0.5759801439422972\n",
      "Mean mse, TP: 15.681293255289784\n",
      "Mean classification error for TP: 0.3289473684210526\n"
     ]
    }
   ],
   "source": [
    "# With wind, predictable nodes\n",
    "fpath = \"../Data/CrossValidation/TP_continuous_fromPredictableNodes.csv\"\n",
    "results_dict = xval_postprocess(\"TP\", fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, TP: 0.7325657242987758\n",
      "Mean mse, TP: 10.76175599303794\n",
      "Mean classification error for TP: 0.3078947368421053\n"
     ]
    }
   ],
   "source": [
    "# Remove wind, all nodes\n",
    "fpath = \"../Data/CrossValidation/TP_cont_fromAllNodes_noWind.csv\"\n",
    "results_dict = xval_postprocess(\"TP\", fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, TP: 0.5792585066304219\n",
      "Mean mse, TP: 15.545155522656668\n",
      "Mean classification error for TP: 0.33421052631578946\n"
     ]
    }
   ],
   "source": [
    "# Remove wind, predictable nodes only\n",
    "fpath = \"../Data/CrossValidation/TP_cont_fromPredictableNodes_noWind.csv\"\n",
    "results_dict = xval_postprocess(\"TP\", fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP: Comparison of classification errors between continuous and discrete networks:\n",
    "\n",
    "Including met.no wind:\n",
    "- Predictable/measurable nodes, discrete BN: 0.55\n",
    "- Predictable/measurable nodes, continuous BN: 0.36\n",
    "- All nodes, discrete BN: 0.59\n",
    "- All nodes, continuous BN: 0.32\n",
    "\n",
    "**Other structures**\n",
    "\n",
    "Remove met.no wind:\n",
    "- All stats improved a bit without wind in continuous network. Therefore **decide to remove wind-TP link**.\n",
    "- **Comparison of discrete and continuous network classification error, without metno wind**, excluding 2019 data, becomes:\n",
    "\n",
    "    - Predictable/measurable nodes, discrete: 0.51\n",
    "    - Predictable/measurable nodes, continuous: 0.33\n",
    "    - All nodes, discrete: 0.53\n",
    "    - All nodes, continuous: 0.31\n",
    "    \n",
    "So take this as the final comparison of discrete vs continuous for TP, and part of the justification for going continuous. Other justification is the cpts in the discrete, which don't show the right behaviour (see markdown comments in BN development notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chl-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, chla: 0.5349801355638153\n",
      "Mean mse, chla: 22.97000935631268\n",
      "Mean classification error for chla: 0.3157894736842105\n"
     ]
    }
   ],
   "source": [
    "fpath = \"../Data/CrossValidation/chla_continuous_fromPredictableNodes.csv\"\n",
    "results_dict = xval_postprocess(\"chla\", fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, chla: 0.7123435737234363\n",
      "Mean mse, chla: 15.572902979921256\n",
      "Mean classification error for chla: 0.24210526315789474\n"
     ]
    }
   ],
   "source": [
    "fpath = \"../Data/CrossValidation/chla_continuous_fromAllNodes.csv\"\n",
    "results_dict = xval_postprocess(\"chla\", fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, chla: 0.5387591347925305\n",
      "Mean mse, chla: 22.790799773849056\n",
      "Mean classification error for chla: 0.3157894736842105\n"
     ]
    }
   ],
   "source": [
    "# Remove wind, predictable nodes\n",
    "fpath = \"../Data/CrossValidation/chla_cont_fromPredictableNodes_noWind.csv\"\n",
    "results_dict = xval_postprocess(\"chla\", fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, chla: 0.7082228333391504\n",
      "Mean mse, chla: 15.675156887827393\n",
      "Mean classification error for chla: 0.21315789473684207\n"
     ]
    }
   ],
   "source": [
    "# Remove wind, all nodes\n",
    "fpath = \"../Data/CrossValidation/chla_cont_fromAllNodes_noWind.csv\"\n",
    "results_dict = xval_postprocess(\"chla\", fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chl-a, comparison of classification error between continuous and discrete\n",
    "\n",
    "Whole network used to predict chl-a node:\n",
    "- discrete BN:0.08\n",
    "- continuous BN: 0.21\n",
    "- continuous, adding extra WFD class (G-M boundary at 10.0): 0.44\n",
    "\n",
    "Only include nodes that will be updated when forecasting:\n",
    "- discrete BN: 0.08\n",
    "- continuous BN: 0.32\n",
    "- continuous, adding extra WFD class (G-M boundary at 10.0): 0.49\n",
    "\n",
    "Lower classification error when using the discrete network. But problems with the discrete chla cpts outweigh these differences:\n",
    "- When chla_prevSummer is high, changing wind speed from L to H doesn't result in any change to probs for low TP. But if TP is high, higher wind speed makes it more likely to have high chla. This isn't right, and is only because of a lack of data points. Therefore would have had to remove the wind-chla link.\n",
    "- Even after doing this, still have probs: when previous summer's chl-a is low, chla is always predicted to be low, and TP has no effect. When previous summer's chl-a is high, the TP effect is the wrong way around: when TP is low, expect high chla. When TP is high, have a lower chance of high TP than when TP is low. Again, just due to low data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other structures\n",
    "\n",
    "- Removing wind: no change or slight deterioration, depending on whether you predict using just predictable nodes (no change) or the whole network (slight deterioration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyano\n",
    "\n",
    "First, need to alter the boundaries in the boundaries dict for cyano, to take account of the box-cox transformation applied to the continuous data:  y* = (y^L - 1)/L, where we used lambda=0.1 when transforming original cyano data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound_dict['cyano'] = boxcox(bound_dict['cyano'], lmbda=0.1)\n",
    "bound_dict['cyano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, cyano: 0.6295420438153104\n",
      "Mean mse, cyano: 1.0102731490753691\n",
      "Mean classification error for cyano: 0.16521739130434784\n"
     ]
    }
   ],
   "source": [
    "fpath = \"../Data/CrossValidation/cyano_continuous_fromPredictableNodes.csv\"\n",
    "results_dict = xval_postprocess(\"cyano\", fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean correlation coefficient, cyano: 0.7839995687523232\n",
      "Mean mse, cyano: 0.6510882952764112\n",
      "Mean classification error for cyano: 0.1782608695652174\n"
     ]
    }
   ],
   "source": [
    "fpath = \"../Data/CrossValidation/cyano_continuous_fromAllNodes.csv\"\n",
    "results_dict = xval_postprocess(\"cyano\", fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of classification error, continuous vs discrete networks:\n",
    "\n",
    "- Predictable/measurable nodes, discrete: 0.23\n",
    "- Predictable/measurable nodes, continuous: 0.17\n",
    "- All nodes, discrete: 0.13\n",
    "- All nodes, continuous: 0.18\n",
    "\n",
    "In this case, the continuous network has lower classification error than the discrete network when using predictable/measurable nodes, but slightly higher error when using all nodes (first time this has been the case). But errors still pretty low, and added benefit of the relationships being sensible between nodes rather than the cpt issue we have for one of the cpts in the cyano class: in the high summer colour class, as chla increases from L to H, the chance of high cyano decreases. I think we would expect positive relationship between chla and cyano regardless of colour, just a lower chance of high cyano when colour is higher. So this is another low data vol artefact.\n",
    "\n",
    "**To investigate in the future:**\n",
    "- Cross val of cyano predictive ability with/without colour-cyano link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed changes to structure based on cross validation results\n",
    "\n",
    "(in this notebook, and the BN_Development_1Season one)\n",
    "\n",
    "- Remove wind-TP link. Keep wind-chla link.\n",
    "- Keep rain-colour link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
